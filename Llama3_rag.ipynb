{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/rag/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/opt/conda/envs/rag/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from secret import HF_TOKEN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig,\n",
    ")\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rag/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.20s/it]\n",
      "/opt/conda/envs/rag/lib/python3.10/site-packages/transformers/utils/hub.py:373: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "llama3 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    device_map = \"auto\",\n",
    "    trust_remote_code = True,\n",
    "    load_in_8bit = True,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "#wrapping the model in pipeline object\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model = llama3,\n",
    "    tokenizer= tokenizer, \n",
    "    temperature = 0.1,\n",
    "    repetition_penalty = 1.2,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "What is a large language model? A large language model (LLM) refers to a type of artificial intelligence (AI) that has been trained on vast amounts of text data. This training enables the LLM to learn patterns, relationships and context within this massive dataset.\n",
       "\n",
       "The primary goal of an LLM is to generate human-like responses or texts based on the input it receives. These inputs can be in the form of prompts, questions, or even entire sentences.\n",
       "\n",
       "In addition to generating human-like responses, LLMS are also capable of performing various natural language processing tasks such as:\n",
       "\n",
       "1. Language translation: The ability to translate text from one language to another.\n",
       "2. Sentiment analysis: The ability to analyze the sentiment or emotional tone behind a piece of text.\n",
       "3. Text classification: The ability to classify pieces of text into predefined categories or classes.\n",
       "4. Named entity recognition: The ability to identify specific entities such as names, locations, organizations, etc. within unstructured text.\n",
       "\n",
       "These capabilities make Large Language Models powerful tools for a wide range of applications including but not limited to customer service chatbots, content generation platforms, language learning systems, and more."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is a large language model?\"\n",
    "out = hf_pipeline(prompt)\n",
    "\n",
    "Markdown(out[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages loaded : 77\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Dataset\n",
       "Num. of\n",
       "Comparisons\n",
       "Avg. # Turns\n",
       "per Dialogue\n",
       "Avg. # Tokens\n",
       "per Example\n",
       "Avg. # Tokens\n",
       "in Prompt\n",
       "Avg. # Tokens\n",
       "in Response\n",
       "Anthropic Helpful\n",
       "122,387\n",
       "3.0\n",
       "251.5\n",
       "17.7\n",
       "88.4\n",
       "Anthropic Harmless\n",
       "43,966\n",
       "3.0\n",
       "152.5\n",
       "15.7\n",
       "46.4\n",
       "OpenAI Summarize\n",
       "176,625\n",
       "1.0\n",
       "371.1\n",
       "336.0\n",
       "35.1\n",
       "OpenAI WebGPT\n",
       "13,333\n",
       "1.0\n",
       "237.2\n",
       "48.3\n",
       "188.9\n",
       "StackExchange\n",
       "1,038,480\n",
       "1.0\n",
       "440.2\n",
       "200.1\n",
       "240.2\n",
       "Stanford SHP\n",
       "74,882\n",
       "1.0\n",
       "338.3\n",
       "199.5\n",
       "138.8\n",
       "Synthetic GPT-J\n",
       "33,139\n",
       "1.0\n",
       "123.3\n",
       "13.0\n",
       "110.3\n",
       "Meta (Safety & Helpfulness)\n",
       "1,418,091\n",
       "3.9\n",
       "798.5\n",
       "31.4\n",
       "234.1\n",
       "Total\n",
       "2,919,326\n",
       "1.6\n",
       "595.7\n",
       "108.2\n",
       "216.9\n",
       "Table 6: Statistics of human preference data for reward modeling. We list both the open-source and\n",
       "internally collected human preference data used for reward modeling. Note that a binary human preference\n",
       "comparison contains 2 responses (chosen and rejected) sharing the same prompt (and previous dialogue).\n",
       "Each example consists of a prompt (including previous dialogue if available) and a response, which is the\n",
       "input of the reward model. We report the number of comparisons, the average number of turns per dialogue,\n",
       "the average number of tokens per example, per prompt and per response. More details on Meta helpfulness\n",
       "and safety data per batch can be found in Appendix A.3.1.\n",
       "knows. This prevents cases where, for instance, the two models would have an information mismatch, which\n",
       "could result in favoring hallucinations. The model architecture and hyper-parameters are identical to those\n",
       "of the pretrained language models, except that the classification head for next-token prediction is replaced\n",
       "with a regression head for outputting a scalar reward.\n",
       "Training Objectives.\n",
       "To train the reward model, we convert our collected pairwise human preference data\n",
       "into a binary ranking label format (i.e., chosen & rejected) and enforce the chosen response to have a higher\n",
       "score than its counterpart. We used a binary ranking loss consistent with Ouyang et al. (2022):\n",
       "Lranking = −log(σ(rθ(x, yc) −rθ(x, yr)))\n",
       "(1)\n",
       "where rθ(x, y) is the scalar score output for prompt x and completion y with model weights θ. yc is the\n",
       "preferred response that annotators choose and yr is the rejected counterpart.\n",
       "Built on top of this binary ranking loss, we further modify it separately for better helpfulness and safety\n",
       "reward models as follows. Given that our preference ratings is decomposed as a scale of four points (e.g.,\n",
       "significantly better), as presented in Section 3.2.1, it can be useful to leverage this information to explicitly\n",
       "teach the reward model to assign more discrepant scores to the generations that have more differences. To\n",
       "do so, we further add a margin component in the loss:\n",
       "Lranking = −log(σ(rθ(x, yc) −rθ(x, yr) −m(r)))\n",
       "(2)\n",
       "where the margin m(r) is a discrete function of the preference rating. Naturally, we use a large margin\n",
       "for pairs with distinct responses, and a smaller one for those with similar responses (shown in Table 27).\n",
       "We found this margin component can improve Helpfulness reward model accuracy especially on samples\n",
       "where two responses are more separable. More detailed ablation and analysis can be found in Table 28 in\n",
       "Appendix A.3.3.\n",
       "Data Composition.\n",
       "We combine our newly collected data with existing open-source preference datasets\n",
       "to form a larger training dataset. Initially, open-source datasets were used to bootstrap our reward models\n",
       "while we were in the process of collecting preference annotation data. We note that in the context of RLHF in\n",
       "this study, the role of reward signals is to learn human preference for Llama 2-Chat outputs rather than\n",
       "any model outputs. However, in our experiments, we do not observe negative transfer from the open-source\n",
       "preference datasets. Thus, we have decided to keep them in our data mixture, as they could enable better\n",
       "generalization for the reward model and prevent reward hacking, i.e. Llama 2-Chat taking advantage of\n",
       "some weaknesses of our reward, and so artificially inflating the score despite performing less well.\n",
       "With training data available from different sources, we experimented with different mixing recipes for both\n",
       "Helpfulness and Safety reward models to ascertain the best settings. After extensive experimentation, the\n",
       "11\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "#PDF loader\n",
    "pdf_loader = PyMuPDFLoader(\"llama2_paper.pdf\")\n",
    "\n",
    "#loading data\n",
    "pages = pdf_loader.load()\n",
    "\n",
    "#observe number of pages loaded\n",
    "print(f\"Number of pages loaded : {len(pages)}\")\n",
    "\n",
    "Markdown(pages[10].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages loaded: 1\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Llama (acronym for Large Language Model Meta AI, and formerly stylized as LLaMA) is a family of autoregressive large language models released by Meta AI starting in February 2023. The latest version is Llama 3 released in April 2024.\n",
       "Model weights for the first version of Llama were made available to the research community under a non-commercial license, and access was granted on a case-by-case basis. Unauthorized copies of the model were shared via BitTorrent, in response, Meta AI issued DMCA takedown requests against repositories sharing the link on GitHub. Subsequent versions of Llama were made accessible outside academia and released under licenses that permitted some commercial use. Llama models are trained at different parameter sizes, typically ranging between 7B and 70B. Originally, Llama was only available as a foundation model. Starting with Llama 2, Meta AI started releasing instruction fine-tuned versions alongside foundation models.\n",
       "Alongside the release of Llama 3, Meta added virtual assistant features to Facebook and WhatsApp in select regions, and a standalone website. Both services use a Llama 3 model.\n",
       "\n",
       "\n",
       "== Background ==\n",
       "After the release of large language model's such as GPT-3, a focus of research was up-scaling models which in some instances showed major increases in emergent capabilities. The release of ChatGPT, and it's surprise success caused an increase in attention to large language models.\n",
       "Compared with other responses to ChatGPT, Meta's Chief AI scientist Yann LeCun stated that large language models are best for aiding with writing.\n",
       "\n",
       "\n",
       "== Initial release ==\n",
       "LLaMA was announced on February 24, 2023, via a blog post and a paper describing the model's training, architecture, and performance. The inference code used to run the model was publicly released under the open-source GPLv3 license. Access to the model's weights was managed by an application process, with access to be granted \"on a case-by-case basis to academic researchers; those affiliated with organizations in government, civil society, and academia; and industry research laboratories around the world\".\n",
       "Llama was trained on only publicly available information, and was trained at various different model sizes, with the intention to make it more accessible to different hardware.\n",
       "Meta AI reported the 13B parameter model performance on most NLP benchmarks exceeded that of the much larger GPT-3 (with 175B parameters), and the largest 65B model was competitive with state of the art models such as PaLM and Chinchilla.\n",
       "\n",
       "\n",
       "=== Leak ===\n",
       "On March 3, 2023, a torrent containing LLaMA's weights was uploaded, with a link to the torrent shared on the 4chan imageboard and subsequently spread through online AI communities. That same day, a pull request on the main LLaMA repository was opened, requesting to add the magnet link to the official documentation. On March 4, a pull request was opened to add links to HuggingFace repositories containing the model. On March 6, Meta filed takedown requests to remove the HuggingFace repositories linked in the pull request, characterizing it as \"unauthorized distribution\" of the model. HuggingFace complied with the requests. On March 20, Meta filed a DMCA takedown request for copyright infringement against a repository containing a script that downloaded LLaMA from a mirror, and GitHub complied the next day.\n",
       "Reactions to the leak varied. Some speculated that the model would be used for malicious purposes, such as more sophisticated spam. Some have celebrated the model's accessibility, as well as the fact that smaller versions of the model can be run relatively cheaply, suggesting that this will promote the flourishing of additional research developments. Multiple commentators, such as Simon Willison, compared LLaMA to Stable Diffusion, a text-to-image model which, unlike comparably sophisticated models which preceded it, was openly distributed, leading to a rapid proliferation of associated tools, techniques, and software.\n",
       "\n",
       "\n",
       "="
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "#defining wikipedia loader\n",
    "wiki_loader = WikipediaLoader(query = \"llama language model\", load_max_docs=1)\n",
    "\n",
    "#loading from wiki based in query\n",
    "wiki_doc = wiki_loader.load()\n",
    "\n",
    "print(f\"Number of pages loaded: {len(wiki_doc)}\")\n",
    "\n",
    "#observe the retrieved page\n",
    "Markdown(wiki_doc[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking/Document Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num splits: 319\n",
      "Content length 968\n",
      "Not everyone who uses AI models has good intentions, and conversational AI agents could potentially be\n",
      "used for nefarious purposes such as generating misinformation or retrieving information about topics like\n",
      "bioterrorism or cybercrime. We have, however, made efforts to tune the models to avoid these topics and\n",
      "diminish any capabilities they might have offered for those use cases.\n",
      "While we attempted to reasonably balance safety with helpfulness, in some instances, our safety tuning goes\n",
      "too far. Users of Llama 2-Chat may observe an overly cautious approach, with the model erring on the side\n",
      "of declining certain requests or responding with too many safety details.\n",
      "Users of the pretrained models need to be particularly cautious, and should take extra steps in tuning and\n",
      "deployment as described in our Responsible Use Guide. §§\n",
      "5.3\n",
      "Responsible Release Strategy\n",
      "Release Details.\n",
      "We make Llama 2 available for both research and commercial use at https://ai.meta.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import random\n",
    "\n",
    "recur_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap=100,\n",
    "    separators = [\"\\n\\n\", \"\\n\", \"\\.\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "#splitting the text\n",
    "data_splits = recur_splitter.split_documents(pages)\n",
    "print(f\"Num splits: {len(data_splits)}\")\n",
    "\n",
    "#checking a random chunk\n",
    "content = random.choice(data_splits).page_content\n",
    "print(\"Content length\", len(content))\n",
    "print(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing sentence transformers\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\":\"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\":False}\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using chroma DB as vectore store\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "#define location for data store\n",
    "persist_directory = \"./vector_store\"\n",
    "\n",
    "#generate and store embeddings\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=data_splits, \n",
    "    embedding=hf_embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retrieved Chunks</th>\n",
       "      <th>Retrieved Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>et al., 2023). For AGI Eval, we only evaluate on the English tasks and report the average.\\nAs shown in Table 3, Llama 2 models outperform Llama 1 models. In particular, Llama 2 70B improves the\\nresults on MMLU and BBH by ≈5 and ≈8 points, respectively, compared to Llama 1 65B. Llama 2 7B and 30B\\nmodels outperform MPT models of the corresponding size on all categories besides code benchmarks. For the\\nFalcon models, Llama 2 7B and 34B outperform Falcon 7B and 40B models on all categories of benchmarks.\\nAdditionally, Llama 2 70B model outperforms all open-source models.\\nIn addition to open-source models, we also compare Llama 2 70B results to closed-source models. As shown\\nin Table 4, Llama 2 70B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant\\ngap on coding benchmarks. Llama 2 70B results are on par or better than PaLM (540B) (Chowdhery et al.,\\n2022) on almost all benchmarks. There is still a large gap in performance between Llama 2 70B and GPT-4</td>\n",
       "      <td>0.504791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>et al., 2020), Big Bench Hard (BBH) (3 shot) (Suzgun et al., 2022), and AGI Eval (3–5 shot) (Zhong\\net al., 2023). For AGI Eval, we only evaluate on the English tasks and report the average.\\nAs shown in Table 3, Llama 2 models outperform Llama 1 models. In particular, Llama 2 70B improves the\\nresults on MMLU and BBH by ≈5 and ≈8 points, respectively, compared to Llama 1 65B. Llama 2 7B and 30B\\nmodels outperform MPT models of the corresponding size on all categories besides code benchmarks. For the\\nFalcon models, Llama 2 7B and 34B outperform Falcon 7B and 40B models on all categories of benchmarks.\\nAdditionally, Llama 2 70B model outperforms all open-source models.\\nIn addition to open-source models, we also compare Llama 2 70B results to closed-source models. As shown\\nin Table 4, Llama 2 70B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant\\ngap on coding benchmarks. Llama 2 70B results are on par or better than PaLM (540B) (Chowdhery et al.,</td>\n",
       "      <td>0.390920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>et al., 2020), Big Bench Hard (BBH) (3 shot) (Suzgun et al., 2022), and AGI Eval (3–5 shot) (Zhong\\net al., 2023). For AGI Eval, we only evaluate on the English tasks and report the average.\\nAs shown in Table 3, Llama 2 models outperform Llama 1 models. In particular, Llama 2 70B improves the\\nresults on MMLU and BBH by ≈5 and ≈8 points, respectively, compared to Llama 1 65B. Llama 2 7B and 30B\\nmodels outperform MPT models of the corresponding size on all categories besides code benchmarks. For the\\nFalcon models, Llama 2 7B and 34B outperform Falcon 7B and 40B models on all categories of benchmarks.\\nAdditionally, Llama 2 70B model outperforms all open-source models.\\nIn addition to open-source models, we also compare Llama 2 70B results to closed-source models. As shown\\nin Table 4, Llama 2 70B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant\\ngap on coding benchmarks. Llama 2 70B results are on par or better than PaLM (540B) (Chowdhery et al.,</td>\n",
       "      <td>0.390920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>guide¶ and code examples‖ to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of\\nour responsible release strategy can be found in Section 5.3.\\nThe remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology\\n(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\\nwork (Section 6), and conclusions (Section 7).\\n‡https://ai.meta.com/resources/models-and-libraries/llama/\\n§We are delaying the release of the 34B model due to a lack of time to sufficiently red team.\\n¶https://ai.meta.com/llama\\n‖https://github.com/facebookresearch/llama\\n4</td>\n",
       "      <td>0.381227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Retrieved Chunks  \\\n",
       "0  et al., 2023). For AGI Eval, we only evaluate on the English tasks and report the average.\\nAs shown in Table 3, Llama 2 models outperform Llama 1 models. In particular, Llama 2 70B improves the\\nresults on MMLU and BBH by ≈5 and ≈8 points, respectively, compared to Llama 1 65B. Llama 2 7B and 30B\\nmodels outperform MPT models of the corresponding size on all categories besides code benchmarks. For the\\nFalcon models, Llama 2 7B and 34B outperform Falcon 7B and 40B models on all categories of benchmarks.\\nAdditionally, Llama 2 70B model outperforms all open-source models.\\nIn addition to open-source models, we also compare Llama 2 70B results to closed-source models. As shown\\nin Table 4, Llama 2 70B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant\\ngap on coding benchmarks. Llama 2 70B results are on par or better than PaLM (540B) (Chowdhery et al.,\\n2022) on almost all benchmarks. There is still a large gap in performance between Llama 2 70B and GPT-4   \n",
       "1       et al., 2020), Big Bench Hard (BBH) (3 shot) (Suzgun et al., 2022), and AGI Eval (3–5 shot) (Zhong\\net al., 2023). For AGI Eval, we only evaluate on the English tasks and report the average.\\nAs shown in Table 3, Llama 2 models outperform Llama 1 models. In particular, Llama 2 70B improves the\\nresults on MMLU and BBH by ≈5 and ≈8 points, respectively, compared to Llama 1 65B. Llama 2 7B and 30B\\nmodels outperform MPT models of the corresponding size on all categories besides code benchmarks. For the\\nFalcon models, Llama 2 7B and 34B outperform Falcon 7B and 40B models on all categories of benchmarks.\\nAdditionally, Llama 2 70B model outperforms all open-source models.\\nIn addition to open-source models, we also compare Llama 2 70B results to closed-source models. As shown\\nin Table 4, Llama 2 70B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant\\ngap on coding benchmarks. Llama 2 70B results are on par or better than PaLM (540B) (Chowdhery et al.,   \n",
       "2       et al., 2020), Big Bench Hard (BBH) (3 shot) (Suzgun et al., 2022), and AGI Eval (3–5 shot) (Zhong\\net al., 2023). For AGI Eval, we only evaluate on the English tasks and report the average.\\nAs shown in Table 3, Llama 2 models outperform Llama 1 models. In particular, Llama 2 70B improves the\\nresults on MMLU and BBH by ≈5 and ≈8 points, respectively, compared to Llama 1 65B. Llama 2 7B and 30B\\nmodels outperform MPT models of the corresponding size on all categories besides code benchmarks. For the\\nFalcon models, Llama 2 7B and 34B outperform Falcon 7B and 40B models on all categories of benchmarks.\\nAdditionally, Llama 2 70B model outperforms all open-source models.\\nIn addition to open-source models, we also compare Llama 2 70B results to closed-source models. As shown\\nin Table 4, Llama 2 70B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant\\ngap on coding benchmarks. Llama 2 70B results are on par or better than PaLM (540B) (Chowdhery et al.,   \n",
       "3                                                                                                                                                                                                                                                                                                                                                    guide¶ and code examples‖ to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of\\nour responsible release strategy can be found in Section 5.3.\\nThe remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology\\n(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\\nwork (Section 6), and conclusions (Section 7).\\n‡https://ai.meta.com/resources/models-and-libraries/llama/\\n§We are delaying the release of the 34B model due to a lack of time to sufficiently red team.\\n¶https://ai.meta.com/llama\\n‖https://github.com/facebookresearch/llama\\n4   \n",
       "\n",
       "   Retrieved Score  \n",
       "0         0.504791  \n",
       "1         0.390920  \n",
       "2         0.390920  \n",
       "3         0.381227  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "#query to retrieve similar chunks\n",
    "query = \"How is the Llama model comparing to GPT-4?\"\n",
    "\n",
    "#retrieving similar chunks based on relevance\n",
    "similar_chunks = vectordb.similarity_search_with_relevance_scores(query, k=4)\n",
    "\n",
    "#format dcoument to text text format\n",
    "retrieved_text = [chunk[0].page_content for chunk in similar_chunks]\n",
    "relevance_score = [chunk[1] for chunk in similar_chunks]\n",
    "\n",
    "retrieved_chunks = pd.DataFrame(\n",
    "    list(zip(retrieved_text, relevance_score)),\n",
    "    columns=[\"Retrieved Chunks\", \"Retrieved Score\"]\n",
    ")   \n",
    "\n",
    "display(retrieved_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How is the Llama model in comparison to GPT-4',\n",
       " 'result': \"The Llama 2 70B model has been demonstrated to be highly competitive with state-of-the-art language models like GPT-4.\\n\\nHowever, it's important to note that while Llama 2 70B may not have achieved the same level of performance as GPT-4, it is still a very impressive achievement for a model of its size and complexity.\\n\\nIt's worth noting that the development of AI models like Llama 2 70B and GPT-4 is an ongoing process, and future advancements could potentially lead to even more impressive achievements in the field of natural language processing.\",\n",
       " 'source_documents': [Document(page_content='et al., 2023). For AGI Eval, we only evaluate on the English tasks and report the average.\\nAs shown in Table 3, Llama 2 models outperform Llama 1 models. In particular, Llama 2 70B improves the\\nresults on MMLU and BBH by ≈5 and ≈8 points, respectively, compared to Llama 1 65B. Llama 2 7B and 30B\\nmodels outperform MPT models of the corresponding size on all categories besides code benchmarks. For the\\nFalcon models, Llama 2 7B and 34B outperform Falcon 7B and 40B models on all categories of benchmarks.\\nAdditionally, Llama 2 70B model outperforms all open-source models.\\nIn addition to open-source models, we also compare Llama 2 70B results to closed-source models. As shown\\nin Table 4, Llama 2 70B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant\\ngap on coding benchmarks. Llama 2 70B results are on par or better than PaLM (540B) (Chowdhery et al.,\\n2022) on almost all benchmarks. There is still a large gap in performance between Llama 2 70B and GPT-4', metadata={'author': '', 'creationDate': 'D:20230720003036Z', 'creator': 'LaTeX with hyperref', 'file_path': 'llama2_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20230720003036Z', 'page': 7, 'producer': 'pdfTeX-1.40.25', 'source': 'llama2_paper.pdf', 'subject': '', 'title': '', 'total_pages': 77, 'trapped': ''})]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "import warnings\n",
    "\n",
    "#supressing warings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#prompt template for QA\n",
    "qa_template = \"\"\" USE the given context to answer the question.\n",
    "If you don't know the answer just say that you don't know, don't try to create an answer.\n",
    "Keep the asnwers as concise as possible\n",
    "\n",
    "Context : {context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt_template = PromptTemplate.from_template(qa_template)\n",
    "llama3_pipeline = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "#defining retrieval Q&A stuff chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llama3_pipeline,\n",
    "    retriever = vectordb.as_retriever(search_kwargs={'k':1}),\n",
    "    return_source_documents = True,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\":qa_prompt_template}\n",
    ")\n",
    "\n",
    "#perform retrieval Q&A\n",
    "qa_response = qa_chain({\"query\": \"How is the Llama model in comparison to GPT-4\"})\n",
    "\n",
    "qa_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The Llama 2 70B model has been demonstrated to be highly competitive with state-of-the-art language models like GPT-4.\n",
       "\n",
       "However, it's important to note that while Llama 2 70B may not have achieved the same level of performance as GPT-4, it is still a very impressive achievement for a model of its size and complexity.\n",
       "\n",
       "It's worth noting that the development of AI models like Llama 2 70B and GPT-4 is an ongoing process, and future advancements could potentially lead to even more impressive achievements in the field of natural language processing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(qa_response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
